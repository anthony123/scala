{"name":"A new backend and optimizer for scalac","tagline":"Faster compilation, faster runs","body":"# 1 Goodies\r\n\r\nThe experimental branch `GenRefactored7` of `scalac` bundles improvements in three areas: optimizer, code emitter, and AST-level representation of closures. These improvements make the compiler faster and the emitted code more compact, using all along the standard library available in the distribution.\r\n\r\n## 1.1 Why a new optimizer\r\n\r\n> Aren't _next-silver-bullet_ going to give us top performance, with zero effort required from us?\r\n\r\nThe experimental optimizer instead:\r\n* improves performance today (you know, just in case method handles in JDK 7 remain slow and buggy, for lack of backporting LambdaForm fixes from JDK 8)\r\n* while leaving a door open to future developments (closures via method handles are _also_ optimized)\r\n* consisting of many focused, individually simple, transformations; that are combined to implement bytecode-level refactorings, using ASM http://asm.ow2.org/\r\n\r\n## 1.2 And a faster code emitter, too\r\n\r\nBefore the new optimizer runs (`GenBCodeOpt`) a bytecode emitter (`GenBCode`) turns Abstract Syntax Trees directly into ASM Trees, outperforming by 30% its existing counterpart (`GenICode + GenASM`).\r\n- the intermediate step to build Control Flow Graphs is not needed,\r\n- disk writing and class file building overlap (the more source files, the larger the speedup)\r\n\r\n## 1.3 Leaner Closure ASTs\r\n\r\n**N.B.: MethodHandles are work in progress, the rest works already.**\r\n\r\nThe release version of `scalac` processes closures by creating inner classes early in the compilation pipeline (the \"traditional\" approach to closure conversion). In this prototype, the creation of AST nodes for closures is postponed, simplifying the job of specialization, erasure, and other compiler phases. \r\n\r\nUnder \"modern\" closure conversion, the  bytecode emitter takes responsibility for producing the JVM-level representation of closures. The implementation-level representation can be chosen among:\r\n- inner classes, when using `-closurify:delegating` (yes, like under `-closurify:traditional`, with the big difference that additional optimizations are applicable).\r\n- JSR-292 Method Handles, when using `-closurify:MH`. This option requires `-target:jvm-1.7`. Right now scalac doesn't produce the newest class file format that JDK8 will introduce. In the meantime,  `-target:jvm-1.7` class files can also be used on that platform (JDK 8 is better at JSR-292 in terms of performance and bug fixes). \r\n\r\n\r\n# 2 What's new in the new optimizer\r\n\r\n## 2.1 Inlining\r\n\r\nThe inliner currently used in scalac has a few problems:\r\n\r\n* closure elimination is implemented as repeated method inlinings. Upon being forced to stop doing that (e.g., recursive method) none of the previous inlinings is undone, leaving both the closure class and a trail of duplicate code.\r\n* code may be inlined from third-party libraries or the JDK. In general methods not marked `@inline` may be inlined as discussed in thread [the perils of inlining](https://groups.google.com/d/topic/scala-internals/uyFWFRbUD0o/discussion) \r\n* invocation cycles (ie M1() calling M2() calling M1() etc) are \"broken\" only after hitting the maximum method size threshold, leaving a trail of duplicate code behind.\r\n\r\nInstead, the experimental optimizer just follows a simple principle:\r\n\r\n> only inline @inline-marked methods, and always inline them, including under separate-compilation\r\n\r\nThus the new inliner is deterministic, not dependent on heuristics about method sizes or similar. The only additional requirement (if you will) is that the method to dispatch (the one marked `@inline`) can be found via the static type of the receiver, e.g. in a `Range.foreach()` callsite the type of the receiver must be `Range` or subtype (in general, not a super type where the `@inline` method is defined). After all, inlining is a conscious decision: making that explicit via the type of the receiver is straightforward. As a result, the `@noinline` annotation doesn't play a role anymore.\r\n\r\nThe new optimizer provides detailed logging about performed inlinings, as well as diagnostics when inlining proves unfeasible (down to the culprit bytecode instructions). With that, fixing the causes of non-inlining takes way less effort, as the following shows.\r\n\r\nLog example:\r\n\r\n    [log jvm] Successful closure-inlining (albeit null receiver couldn't be ruled out). Callsite: \r\n      scala/tools/nsc/Global.exitingTyper(Lscala/Function0;)Ljava/lang/Object;\r\n    occurring in method\r\n      scala/tools/nsc/interpreter/JLineCompletion$CompilerCompletion$class::memberNamed(Lscala/tools/nsc/interpreter/JLineCompletion$CompilerCompletion;Ljava/lang/String;)Lscala/reflect/internal/Symbols$Symbol;\r\n\r\n\r\nWarning example:\r\n\r\n    SpecializeTypes.scala:1166: warning: Closure-inlining failed because\r\n      scala/collection/immutable/List::mapConserve(Lscala/Function1;)Lscala/collection/immutable/List;\r\n    contains instruction \r\n      INVOKESPECIAL scala/collection/immutable/List.loop$1 (Lscala/collection/mutable/ListBuffer;Lscala/collection/immutable/List;Lscala/collection/immutable/List;Lscala/Function1;)Lscala/collection/immutable/List;\r\n    that would cause IllegalAccessError from class scala/tools/nsc/transform/SpecializeTypes\r\n            val parents1 = parents mapConserve specializedType\r\n                                   ^\r\n\r\nThe warning makes sense: `loop()` is a local method:\r\n\r\n```scala\r\n// scala.collection.immutable.List\r\n  @inline final def mapConserve[B >: A <: AnyRef](f: A => B): List[B] = {\r\n    @tailrec\r\n    def loop(mapped: ListBuffer[B], unchanged: List[A], pending: List[A]): List[B] =\r\n       ...\r\n```\r\n\r\nThe bytecode-level counterpart, `loop$1()`, was emitted as private, as javap output shows:\r\n\r\n```javap\r\nprivate final\r\nscala.collection.immutable.List\r\nloop$1(scala.collection.mutable.ListBuffer,\r\n       scala.collection.immutable.List,\r\n       scala.collection.immutable.List,\r\n       scala.Function1);\r\n  ...\r\n```\r\n\r\n## 2.2 Final methods in traits as fast as invokestatic, amenable to inlining too\r\n\r\nWithout any developer action, a final method in a trait is invoked via `invokestatic` (before: virtual dispatch followed by forwarding to implementation class). \r\n\r\nAdditionally, `@inline` can also be used. In the example below, the callsite `y.m(7)` in method `host()` is inlined, in spite of the receiver having a (static) trait type.\r\n\r\n```scala\r\ntrait T {\r\n  @inline final def m(x: Int) { println(x) }\r\n}\r\nclass C {\r\n  def host(y: T) {\r\n    y.m(7)\r\n  }\r\n}\r\n```\r\n\r\n## 2.3 GC-savvy closures: Singleton closures, Minimization of closure state\r\n\r\nSome anonymous closures depend only on `apply()` arguments, for example the char filter function:\r\n\r\n```scala\r\n  def deeplyNestedMethod(str: String) = {\r\n    str filter { (char: Char) => (char >= 'a' && char <= 'f') || (char >= 'A' && char <= 'F') || (char >= '0' && char <= '9') }\r\n  }\r\n```\r\n\r\nIn these cases, the new optimizer avoids repeated allocations by keeping (in a static field) a singleton-instance that is reused.\r\n\r\nAfter dead-code elimination, closure state comprises only what is actually accessed.\r\n\r\nThese features are more useful on Android (besides micro-benchmarks) where a vast RAM doesn't masquerade redundant allocations.\r\n\r\n## 2.4 Supported optimizations\r\n\r\n### 2.4.1 Intra-method optimizations\r\n\r\n* collapse a multi-jump chain to target its final destination via a single jump\r\n* remove unreachable code\r\n* remove those LabelNodes and LineNumbers that aren't in use\r\n* remove dangling exception handlers\r\n* copy propagation\r\n* dead-store elimination\r\n* Preserve side-effects, but remove those (producer, consumer) pairs where the consumer is a DROP and\r\n  the producer has its value consumed only by the DROP in question.\r\n* simplify branches that need not be taken to get to their destination.\r\n* nullness propagation\r\n* constant folding\r\n* caching repeatable reads of stable values\r\n* eliding box/unbox pairs\r\n* eliding redundant local vars\r\n\r\n\r\n### 2.4.2 Intra-class optimizations\r\n\r\n* those private members of a class which see no use are elided\r\n* tree-shake unused closures, minimize the fields of those remaining\r\n* minimization of closure-allocations\r\n* refresh the InnerClasses JVM attribute\r\n\r\n### 2.4.3 Whole-program optimizations\r\n\r\n* method inlining\r\n* closure inlining\r\n\r\n\r\n\r\n# 3 Test driving the new optimizer\r\n\r\n## 3.1 How much does it add to compilation time?\r\n\r\nThe new optimizer (except the whole-program step) is task-parallel:\r\n* intra-method optimizations are run in parallel for different methods;\r\n* intra-class optimizations are run parallel for different classes\r\n\r\nVisually:\r\n\r\n![experimental optimizer uses task parallelism](http://lampwww.epfl.ch/~magarcia/ScalaCompilerCornerReloaded/yk.png)\r\n\r\nThere's no reason to limit the worker pool to 8 threads, that's \"configurable\" at:\r\n\r\n```scala\r\n    val MAX_THREADS = scala.math.min(\r\n      32,\r\n      java.lang.Runtime.getRuntime.availableProcessors\r\n    )\r\n```\r\n\r\n## 3.2 Emitted code size\r\n\r\nLet's take scala/scala as case study, compiling src/compiler and src/reflect together using:\r\n\r\n1. \"Old\": GenICode, the current optimizer, and GenASM.\r\n2. \"New\": GenBCode and the experimental optimizer.\r\n\r\nThe new optimizer produces smaller JARs:\r\n\r\n1. `\"Old\": Total length 35'731'733, packed length 13'981'556`\r\n2. `\"New\": Total length 30'533'362, packed length 12'191'163`\r\n\r\nThe above reflects not so much code reductions (in terms of instruction count) but smaller constant pools due to Lean Closure Classes (an LCC just delegates to the class where the anon-closure is instantiated, which usually already has the constant pool entries that under \"traditional\" closure conversion have to be duplicated in the anon-closure-class).\r\n\r\nIf you're after code size reductions as well, `-neo:o1` (intra-method optimizations) or up will do the trick. Moreover, on a multicore `-neo:o1` doesn't increase appreciably compilation time, I'm actually thinking making it default.\r\n\r\nFor example, method `driver()` in `test/files/run/t7181.scala` results in:\r\n* 881 instructions, when compiled with `-neo:o2 -closurify:delegating`\r\n* 1004 instructions, when compiled with `-neo:GenASM -closurify:traditional -optimise` (actually, `-optimise` increases code size, but the comparison against `-neo:o2` is fair).\r\n\r\nThat's 881 vs 1004 instructions and not bytes, which still matters in case you're the one who has to read 881 vs 1004 lines of javap output.\r\n\r\n## 3.3 Benchmarks\r\n\r\nFeedback is welcome!\r\n\r\n## 3.4 Speeding up scalac\r\n\r\nSpeedups in the range 10% to 20% have been observed against the latest `scalac`. The upper range is achieved by having a compiler optimized by the new optimizer compile using `-neo:GenBCode -closurify:delegating` (ie unoptimized compilation, using the new bytecode emitter and Leaner Closure ASTs). Just one data point:\r\n\r\n```\r\n[stopwatch] [locker.comp.timer: 1:43.359 sec]\r\n...\r\n[stopwatch] [quick.comp.timer:  1:32.335 sec]\r\n```\r\n\r\nRight now the new optimizer alone makes scalac only marginally faster. Instead, the 15% speedup mentioned above is due to the compiler doing less work: (a) Leaner Closure ASTs, (b) the new bytecode emitter, and (c) lower GC overhead. That's normal: `scalac` is dominated by factors not optimizable (at least not in the short term). Examples abound: \r\n* millions of `::` allocated. Neither the old nor the new optimizer are tuned to reduce that.\r\n* for deeper insights:  https://github.com/gkossakowski/scalac-aspects\r\n* actually, tools like Caliper or ScalaMeter, by themselves, tell how much faster sthg runs. When building an optimizer, it's more useful to know \"why\" stgh runs faster (specially with 20+ optimizations at play). With some work, the toolset that Grzegorz has jumpstarted can provide those insights. \r\n\r\nThe new optimizer may well make *your* code run faster. To find out, give it a try.\r\n\r\n## 3.5 Bugs fixed\r\n\r\n* [SI-4767](https://issues.scala-lang.org/browse/SI-4767) Methods defined in traits are not inlined \r\n* [SI-5850](https://issues.scala-lang.org/browse/SI-5850) Inlined code shouldn't forget null-check on the original receiver\r\n* [SI-5950](https://issues.scala-lang.org/browse/SI-5950) inlining creates duplicates when anon-closure-class can't be eliminated after all\r\n* [SI-6546](https://issues.scala-lang.org/browse/SI-6546) Optimizer leaves references to classes that have been eliminated by inlining \r\n* [SI-6720](https://issues.scala-lang.org/browse/SI-6720) Uninitialized object exists on backward branch\r\n* [SI-6759](https://issues.scala-lang.org/browse/SI-6759) seek clarification about necessary and sufficient conditions for inclusion in InnerClasses JVM attribute \r\n* all `ICodeReader` bugs (not used anymore)\r\n\r\n\r\n# 4 Getting Started\r\n\r\nThe first step is checking out branch `GenRefactored7` of repository https://github.com/magarciaEPFL/scala\r\n\r\n```\r\ngit clone git://github.com/magarciaEPFL/scala.git GenRefactored7\r\ncd GenRefactored7\r\ngit checkout -b GenRefactored7 origin/GenRefactored7\r\nant all.clean && ant\r\n```\r\n\r\nAfter `ant all.clean && ant` , a few knobs allow fine tuning: \r\n- fine grained optimization levels (shown below) to progressively move into more advanced (more recent, less tested) optimizations.\r\n- backdoor to use the \"old\" optimizer and backend, to ease comparison. All options of the \"old\" optimizer are available in this case ( `-optimise` `-Yinline`  `-Yinline-handlers`  `-Yclosure-elim`  and `-Ydead-code` ).\r\n\r\n## 4.1 Choosing optimization level via compiler flags\r\n\r\nEach optimization level includes all optimizations from lower levels. Levels `o1` and up activate the new code emitter (`GenBCode`):\r\n\r\n<table>\r\n    <tr> <td><code>-neo:GenASM</code></td>   <td>backdoor flag to use the old backend</td> </tr>\r\n    <tr> <td><code>-neo:GenBCode</code></td> <td>use the new code emitter, just emitting trees as delivered by CleanUp</td> </tr>\r\n    <tr> <td><code>-neo:o1</code></td>       <td>Intra-method optimizations only, ie no inlining, no closure optimizations</td> </tr>\r\n    <tr> <td><code>-neo:o2</code></td>       <td>Method inlining and closure stack-allocation, without \"advanced\" closure optimization</td> </tr>\r\n    <tr> <td><code>-neo:o3</code></td>       <td>\"Advanced\" closure optimization: minimization of closure state, \"singletonize\" closures</td> </tr>\r\n    <tr> <td><code>-neo:o4</code></td>       <td>Rewiring of final methods of traits to directly target them using invokestatic rather than invokeinterface</td> </tr>\r\n</table>\r\n\r\nPro tip: optimizing the compiler and standard library themselves under a level other than the default is as easy as tweaking [this line in `ScalaSettings.scala`](https://github.com/magarciaEPFL/scala/blob/GenRefactored7/src/compiler/scala/tools/nsc/settings/ScalaSettings.scala#L207) followed by `ant all.clean && ant`\r\n\r\n## 4.2 Choosing bytecode-level representation of closures (inner classes vs. method handles)\r\n\r\n**N.B.: MethodHandles are work in progress, the rest works already.**\r\n\r\n<table>\r\n<tr>\r\n<td><code>-closurify:traditional</code></td>   <td>Good ol' dedicated inner class for each closure. Available under GenASM (the only option there), and also with GenBCode but only up to -neo:o1</td> </tr>\r\n<tr>\r\n<td><code>-closurify:delegating</code></td> <td>aka \"Late Closure Classes\" ie their creation is postponed (instead of UnCurry during GenBCode) thus lowering the working set during compilation. Allows closure-related optimizations (all optimization levels supported)</td> </tr>\r\n<tr>\r\n<td><code>-closurify:MH</code></td>       <td>A JSR 292 MethodHandle instance with bound arguments for captured environment (aka \"partial application\")\r\nis given as constructor-argument to a *standard* closure-class\r\n(thus doing away with the need for as many individual classes as closure definitions).\r\nAllows all optimization levels, -target must be jvm-1.7 or higher, and the backend must be GenBCode</td> </tr>\r\n</table>\r\n\r\n\r\nUpon choosing conflicting flags, the compiler will not only call them out but also suggest corrective action. For example, upon choosing `-closurify:traditional -neo:o2` the compiler reports:\r\n\r\n```\r\nerror: Optimization level 2 isn't compatible with -closurify = traditional.\r\nThat optimization level requires --closurify:delegating or --closurify:MH\r\n```\r\n\r\n\r\n## 4.3 Diagnostics \r\n\r\nDiagnostics are displayed via `-Ylog:jvm` , for more details add `-Yinline-warnings` and if that's not enough adding `-Ydebug` will show both the individual bytecode instructions subject of the message as well as a listing of the enclosing method (all in ASM textual format, which is always available unlike `javap`).\r\n\r\nAnother useful flag is `-Ygen-asmp <folder>` which similar to `-Ygen-javap` will emit textual files but in ASM format.\r\n\r\n# 5 Future Work\r\n\r\n1. lower GC overhead in those cases where symbol-ids (Ints) can be used instead of symbols, ie rather than set-of-symbol and symbol-to-symbol map, use instead `Set[Int]` and `Map[Int, Symbol]`. Examples at [down with set-of-symbol](https://groups.google.com/d/topic/scala-internals/leI41TCuTdc/discussion)\r\n2. task-parallelism in `inliningRound()` thus making not just `-neo:o2` faster but higher levels too (inlining is the bottleneck, Levels 3 and 4 are themselves relatively fast).\r\n3. [SI-6941](https://issues.scala-lang.org/browse/SI-6941) Pattern matcher inefficiency for basic constructor patterns\r\n4. [SI-6863](https://issues.scala-lang.org/browse/SI-6863) Remember \"Attempt to entry try block with non-empty stack\" ? To circumvent that limitation, a try-occurrence found at an expression position is \"refactored\" into a dedicated method (see `liftTree()` in `UnCurry`). The performance problem with that are slower compilation (more `Tree` nodes) and slower run time (larger constant pool, more bytecode instructions, more `IntRef`s all over). An alternative approach consists in leaving try's in place, with a post-`Cleanup` step guaranteeing empty-stack-on-try-entry. A proposal (in the form of pseudocode) for that transformation can be found at [this thread](https://groups.google.com/d/msg/scala-internals/VkEL7wOVQpE/aSiNnF3ym-cJ)\r\n\r\n\r\nWhat others are doing:\r\n* [Notes from the JS pit: closure optimization](http://blog.cdleary.com/2010/05/notes-from-the-js-pit-closure-optimization/)\r\n* [Runtime metaprogramming via java.lang.invoke.MethodHandle](http://lamp.epfl.ch/~magarcia/ScalaCompilerCornerReloaded/2012Q2/RuntimeMP.pdf)\r\n* [A comparison of the memory behaviour of Java and Scala programs](http://www.scalabench.org/publications.html)\r\n\r\nSuggestions for improvement need not be \"sophisticate\" to be useful :) For example, it's easy game for GenBCode to reduce:\r\n\r\n    new\t#17; //class scala/runtime/IntRef\r\n    dup\r\n    iconst_0\r\n    invokespecial\t#20; //Method scala/runtime/IntRef.\"<init>\":(I)V\r\n\r\nto just a static invocation (of a factory method added for that common case in `scala.runtime.IntRef`). And so on so forth.\r\n\r\n# 6 Comments, benchmarks, test cases, bug reports, are welcome.\r\n\r\nPlease help us help you.\r\n\r\nMiguel Garcia\r\n[http://lampwww.epfl.ch/~magarcia](http://lampwww.epfl.ch/~magarcia)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}